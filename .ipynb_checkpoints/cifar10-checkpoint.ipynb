{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9545aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "from torchvision.datasets.utils import download_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148cd4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isfile('cifar10.tgz')):download_url('https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdb056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.isdir('data')):tarfile.open('cifar10.tgz',\"r:gz\").extractall(path=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92927bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998218fe",
   "metadata": {},
   "source": [
    "# ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07931a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateset = ImageFolder(r'data\\cifar10\\train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cacd15de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 5\n"
     ]
    }
   ],
   "source": [
    "img, label = dateset[25001]\n",
    "print(img.shape,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ca57642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ea08e",
   "metadata": {},
   "source": [
    "# Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a340834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6ac584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 5000\n"
     ]
    }
   ],
   "source": [
    "val_size = 0.1\n",
    "train_size, test_size = int(len(dateset)*(1-val_size)), int(len(dateset)*val_size)\n",
    "\n",
    "train_ds, test_ds = random_split(dateset,[train_size, test_size])\n",
    "print(len(train_ds),len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ee3a1",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcb58307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a73a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71b42cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 40\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size,shuffle=True, num_workers=4,pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds, batch_size,shuffle=True, num_workers=4,pin_memory=True)\n",
    "print(len(train_dl),len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed885c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "268a26b4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a1774c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28029bde3d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randon_seed = 47\n",
    "torch.manual_seed(randon_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08fedb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29b32d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    # 3 x 32 x 32\n",
    "    nn.Conv2d(3,8,kernel_size=3,stride=1,padding=1), # 32 x 32 x 32\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2,2), # 8 x 16 x 16\n",
    "    nn.Conv2d(8,8,kernel_size=3,stride=1,padding=1), # 8 x 16 x 16\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2048,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(64,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(32,16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(16,10),\n",
    "    nn.ReLU(),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41da3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\pytorch_transformer\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0951, 0.1076, 0.0951,  ..., 0.1073, 0.0989, 0.0951],\n",
      "        [0.0948, 0.1096, 0.0948,  ..., 0.1108, 0.0989, 0.0948],\n",
      "        [0.0952, 0.1097, 0.0947,  ..., 0.1049, 0.0998, 0.0947],\n",
      "        ...,\n",
      "        [0.0955, 0.1059, 0.0955,  ..., 0.1084, 0.0995, 0.0955],\n",
      "        [0.0951, 0.1088, 0.0950,  ..., 0.1091, 0.0976, 0.0950],\n",
      "        [0.0958, 0.1053, 0.0958,  ..., 0.1009, 0.1031, 0.0958]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "for img, labels in train_dl:\n",
    "    print(img.shape)\n",
    "    result = model(img)\n",
    "    print(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7df756bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train in module torch.nn.modules.module:\n",
      "\n",
      "train(mode: bool = True) -> ~T method of torch.nn.modules.container.Sequential instance\n",
      "    Sets the module in training mode.\n",
      "    \n",
      "    This has any effect only on certain modules. See documentations of\n",
      "    particular modules for details of their behaviors in training/evaluation\n",
      "    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "    etc.\n",
      "    \n",
      "    Args:\n",
      "        mode (bool): whether to set training mode (``True``) or evaluation\n",
      "                     mode (``False``). Default: ``True``.\n",
      "    \n",
      "    Returns:\n",
      "        Module: self\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12a0e5bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python\\pytorch_transformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\python\\pytorch_transformer\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\python\\pytorch_transformer\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\python\\pytorch_transformer\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python\\pytorch_transformer\\lib\\site-packages\\torch\\nn\\modules\\conv.py:439\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    437\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    438\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mlist\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mParameter\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[31;1mtuple\u001b[0m, \u001b[32;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "model.train(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9558a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14336272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051516d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ed29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc19e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
